[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1587352689,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sinhaharsh.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Harsh Sinha","type":"authors"},{"authors":["Aditya Mehta","Harsh Sinha","Pratik Narang","Murari Mandal"],"categories":[],"content":"","date":1618892730,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618892730,"objectID":"dbde0e4636b54ac1d5350b0e08eb2cd2","permalink":"https://sinhaharsh.github.io/publication/mehta2020hidegan/","publishdate":"2020-04-20T09:55:30+05:30","relpermalink":"/publication/mehta2020hidegan/","section":"publication","summary":"Haze removal in images captured from a diverse set of scenarios is a very challenging problem. The existing dehazing methods either reconstruct the transmission map or directly estimate the dehazed image in RGB color space. In this paper, we make a first attempt to propose a Hyperspectral-guided Image Dehazing Generative Adversarial Network (HIDEGAN). The HIDEGAN architecture is formulated by designing a enhanced version of CYCLEGAN named R2HCYCLE and an enhanced conditional GAN named H2RGAN. The R2HCYCLE makes use of the hyperspectral-image (HSI) in combination with cycle-consistency and skeleton losses in order to improve the quality of information recovery by analyzing the entire spectrum. The H2RGAN estimates the clean RGB image from the hazy hyperspectral image generated by the R2HCYCLE. The models designed for spatial-spectralspatial mapping generate visually better haze-free images. To facilitate HSI generation, datasets from spectral reconstruction challenge at NTIRE 2018 and NTIRE 2020 are used. A comprehensive set of experiments were conducted on the D-Hazy,and the recent RESIDE-Standard (SOTS), RESIDE-Î² (OTS) and RESIDE-Standard (HSTS) datasets. The proposed HIDEGAN outperforms the existing state-ofthe-art in all these datasets. ","tags":[],"title":"HIDeGAN: A Hyperspectral-guided Image Dehazing GAN","type":"publication"},{"authors":["Harsh Sinha","Sakshi Kalra","Yashvardhan Sharma"],"categories":[],"content":"","date":1586083700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587352689,"objectID":"958c9dfca5b8abf9e936c183c1afa825","permalink":"https://sinhaharsh.github.io/publication/sinha2020text/","publishdate":"2020-04-05T16:18:20+05:30","relpermalink":"/publication/sinha2020text/","section":"publication","summary":"With the widespread use of online social networking websites, user generated stories and social network platform have become critical in news propagation. The web portals are being used to mislead users for political gains. Unreliable information is being shared without any fact-checking. Therefore, there is a dire need for automatic news verification system which can help journalists and the common users from misleading content. In this work, the task is defined as being able to classify a tweet as real or fake. The complexity of natural language constructs along with variegated languages makes this task very challenging. In this work, a deep learning model to learn semantic word embeddings is proposed to handle this complexity. The evaluations on the benchmark dataset (VMU 2015) show that deep learning methods are superior to traditional natural language processing algorithms.","tags":[],"title":"Text-Convolutional Neural Networks for Fake News Detection in Tweets","type":"publication"},{"authors":["Harsh Sinha","Vinayak Awasthi","Pawan K Ajmera"],"categories":[],"content":"","date":1586083577,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587352689,"objectID":"52c1376441af60ebf45282eeafe3d76c","permalink":"https://sinhaharsh.github.io/publication/sinha2020audio/","publishdate":"2020-04-05T16:16:17+05:30","relpermalink":"/publication/sinha2020audio/","section":"publication","summary":"Convolutional Neural Networks (CNNs) work surprisingly well and has helped drastically enhance the state-of-the-art techniques in the domain of image classification. The unprecedented success motivated the application of CNNs to the domain of auditory data. Recent publications suggest Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs) for audio classification. This paper aims to achieve audio classification by representing audio as spectrogram images and then use a CNN-based architecture for classification. The paper presents an innovative strategy for a CNN-based neural architecture that learns a sparse representation imitating the receptive neurons in primary auditory cortex in mammals. The feasibility of the proposed CNN-based neural architecture is assessed for audio classification task on standard benchmark datasets such as Google Speech Commands datasets (GSCv1 and GSCv2) and UrbanSound8K dataset (US8K). The proposed CNN architecture referred to as Braided Convolutional Neural Network (BCNN) achieves 97.15%, 95% and 91.9% average recognition accuracy on GSCv1, GSCv2 and US8K datasets respectively outperforming other deep learning architectures.","tags":[],"title":"Audio Classification using Braided Convolutional Neural Networks","type":"publication"},{"authors":["Harsh Sinha","Pawan K Ajmera"],"categories":[],"content":"","date":1548758928,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587352689,"objectID":"eb989cf61303ce75d6007f0c9f704a18","permalink":"https://sinhaharsh.github.io/publication/sinha2019upgrading/","publishdate":"2020-04-05T16:18:48+05:30","relpermalink":"/publication/sinha2019upgrading/","section":"publication","summary":"Biometrics is being widely accepted for user authentication across the globe. Integration of biometrics in the daily life provokes the need to design secure authentication systems. This study proposes the use of outer ear images as a biometric modality. The comparable complexity between the human outer ear and face in terms of its uniqueness and permanence has increased interest in the use of ear as a biometric. However, similar to face recognition, it poses challenges of variation in illumination, contrast, rotation, scale and pose. Owing to the extensive work in the field of computer vision using convolutional neural networks (CNNs), its feasibility in the field of ear biometrics has been presented in this work. The proposed technique uses a CNN as a feature extractor and a support vector machine (SVM) for the classification task. The joint CNN-SVM framework is used for mapping ear images to random base-n codes. The codes are further hashed using the secure hash algorithm SHA-3 to generate secure ear templates. The feasibility of the proposed technique has been evaluated on annotated web ears dataset. This work demonstrates 12.52% average equal error rate without any image pre-processing, which shows that the proposed approach is promising in the field of secure ear biometrics","tags":[],"title":"Upgrading security and protection in Ear Biometrics","type":"publication"},{"authors":["Harsh Sinha","Raunak Manekar","Yash Sinha","Pawan K Ajmera"],"categories":[],"content":"","date":1540982917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587352689,"objectID":"4fe05005f91229ea44e58b0e0e4510e5","permalink":"https://sinhaharsh.github.io/publication/sinha2019convolutional/","publishdate":"2020-04-05T16:18:37+05:30","relpermalink":"/publication/sinha2019convolutional/","section":"publication","summary":"This paper presents a deep learning approach for ear localization and recognition. The comparable complexity between human outer ear and face in terms of its uniqueness and permanence has increased interest in the use of ear as a biometric. But similar to face recognition, it poses challenges such as illumination, contrast, rotation, scale, and pose variation. Most of the techniques used for ear biometric authentication are based on traditional image processing techniques or handcrafted ensemble features. Owing to extensive work in the field of computer vision using convolutional neural networks (CNNs) and histogram of oriented gradients (HOG), the feasibility of deep neural networks (DNNs) in the field of ear biometrics has been explored in this research paper. A framework for ear localization and recognition is proposed that aims to reduce the pipeline for a biometric recognition system. The proposed framework uses HOG with support vector machines (SVMs) for ear localization and CNN for ear recognition. CNNs combine feature extraction and ear recognition tasks into one network with an aim to resolve issues such as variations in illumination, contrast, rotation, scale, and pose. The feasibility of the proposed technique has been evaluated on USTB III database. This work demonstrates 97.9% average recognition accuracy using CNNs without any image preprocessing, which shows that the proposed approach is promising in the field of biometric recognition","tags":[],"title":"Convolutional Neural Network-Based Human Identification Using Outer Ear Images","type":"publication"},{"authors":["Harsh Sinha","Shivin Shrivastava","Yash Sinha"],"categories":[],"content":"","date":1538390942,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587352689,"objectID":"1328221b6ff9a9cc69ae3cedc10b5e01","permalink":"https://sinhaharsh.github.io/publication/sinha2018studying/","publishdate":"2020-04-05T16:19:02+05:30","relpermalink":"/publication/sinha2018studying/","section":"publication","summary":"According to the theory of Embodied Cognition, our behavior is a result of real-time interaction with surroundings, our cognitive skills, and the nervous system. From this perspective, researchers are considering a learning environment which promotes physical activities to achieve cognitive tasks. Such Natural User Interfaces (NUI) make use of gesture-based sensors like the Microsoft Kinect. Yet we lack in-depth studies of how they improve the learning process. In this paper, we present observations of two deployment studies which focus on different roles that NUI can play as a part of learning activities. We deploy the Kinect based applications:- Yoga Soft: A Digital Yoga Instructor and Mudra: A Kinect based Learning System in real life scenarios. The first study is conducted at residences of preadolescent children in Gurgaon, India. The second study is conducted at an education center specializing in the care of kindergarten children in Pilani, India.","tags":[],"title":"Studying the Role of Kinect as a Multi-Sensory Learning Platform for Children","type":"publication"},{"authors":["Harsh Sinha","Pawan K Ajmera"],"categories":[],"content":"","date":1535021354,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587352689,"objectID":"3659b2bc1a77d7a7e6312a00eb44e762","permalink":"https://sinhaharsh.github.io/publication/sinha2018interweaving/","publishdate":"2020-04-05T16:19:14+05:30","relpermalink":"/publication/sinha2018interweaving/","section":"publication","summary":"The monumental success of Convolutional Neural Networks (CNNs) in the field of image classification has motivated the application of CNNs in the domain of auditory data. Prior works have shown performance of Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs) in the field of Content-based Audio Classification. This paper presents a novel concatenating strategy for a CNN-based neural architecture. The proposed methodology was evaluated for audio classification task using UrbanSound8K dataset (US8K) as benchmark. The proposed architecture achieves an average recognition accuracy of 97.55 %, an average EER of  .14% on US8K dataset. A small-footprint variant of the proposed architecture is also proposed. ","tags":[],"title":"Interweaving Convolutions: An application to Audio Classification","type":"publication"}]